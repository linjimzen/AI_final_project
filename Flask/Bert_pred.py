# -*- coding: utf-8 -*-
"""Summary.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Yozqy87RSumOF8fLn84BtWGUbS__XawY
"""

# laod from weights and model
from keras.models import load_model
from transformers import TFBertModel
from transformers import BertTokenizer
import tensorflow as tf
import time
import numpy as np
import os


bert_path = 'best_Bert_weights.h5'

bert_size = os.path.getsize(bert_path)

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)

Maximum_Length = 64
def bert_encode(data,maximum_length) :
    input_ids = []
    attention_masks = []


    for i in range(len(data)):
        encoded = tokenizer.encode_plus(
          data[i],
          add_special_tokens=True, # add  [CLS] or [SEP]
          max_length=maximum_length,
          padding='max_length',
          truncation=True,
          return_attention_mask=True,
        )

        input_ids.append(encoded['input_ids'])
        attention_masks.append(encoded['attention_mask'])
    return np.array(input_ids),np.array(attention_masks)

# complexity - BERT
new_model = load_model(bert_path, custom_objects={'TFBertModel': TFBertModel})

print(new_model.summary())
start_time = time.time()
def pred(input):
    # Tokenize the input sentence into a word list, then encode each word in this list
    result_input_ids,result_attention_masks = bert_encode([input],Maximum_Length)
    # To tesnor format
    input_ids = tf.constant(result_input_ids)
    attention_mask = tf.constant(result_attention_masks)

    # Feed the encoded lsit into model and predict
    label_map = {0: "Negative", 1: "Positive"}
    prob = new_model([input_ids, attention_mask])
    predicted_labels = (prob.numpy() > 0.5).astype(int)

    for i, label in enumerate(predicted_labels):
        print("Predicted label:", label_map[label[0]], "\n")
        
    return label_map[label[0]]

Bert_time = time.time()-start_time

print(f"Total execution time of Bert: {Bert_time} seconds")

print(f"Bert Model Size: {bert_size} bytes")


