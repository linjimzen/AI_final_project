{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de4cb08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# laod from weights and model\n",
    "from keras.models import load_model\n",
    "from transformers import TFBertModel,TFDistilBertModel\n",
    "from transformers import BertTokenizer,DistilBertTokenizer\n",
    "import tensorflow as tf\n",
    "import time \n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "test1 = \"Chefs are nutters. They’re all self-obsessed, delicate, dainty, insecure little souls, and absolute psychopaths. Every last one of them. My gran could do better! And she’s dead!\"\n",
    "test2 = \"This dish, my dear friend, is nothing short of a culinary catastrophe. I've never encountered anything quite so disastrous in all my years in the kitchen. The flavors are an absolute train wreck, and the texture is reminiscent of a soggy, overcooked sponge. If this were a competition, it would undoubtedly be a contestant for the 'Worst Dish of the Century' award. An absolute travesty to the world of cuisine!\"\n",
    "test3 = \"The culinary delights on offer here are simply extraordinary, with each bite offering an explosion of exquisite flavors that dance harmoniously on the palate, making it an unforgettable and truly gastronomic experience.\"\n",
    "\n",
    "bert_path = './best_Bert_weights.h5'\n",
    "distillbert_path = './best_DistillBert_weights.h5'\n",
    "\n",
    "bert_size = os.path.getsize(bert_path)\n",
    "distillbert_size = os.path.getsize(distillbert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9717e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True) \n",
    "Maximum_Length = 64\n",
    "def bert_encode(data,maximum_length) :\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "  \n",
    "\n",
    "    for i in range(len(data)):\n",
    "        encoded = tokenizer.encode_plus(\n",
    "          data[i],\n",
    "          add_special_tokens=True, # add  [CLS] or [SEP]\n",
    "          max_length=maximum_length,\n",
    "          padding='max_length',\n",
    "          truncation=True,\n",
    "          return_attention_mask=True,\n",
    "        )\n",
    "        \n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return np.array(input_ids),np.array(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e0dfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function input_processing at 0x0000018F3A378948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: '<' not supported between instances of 'Literal' and 'str'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function input_processing at 0x0000018F3A378948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: '<' not supported between instances of 'Literal' and 'str'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From C:\\Users\\WTMH\\anaconda3\\envs\\tensor_gpu_py37\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "Text: 'Chefs are nutters. They’re all self-obsessed, delicate, dainty, insecure little souls, and absolute psychopaths. Every last one of them. My gran could do better! And she’s dead!'\n",
      "Predicted label: Negative \n",
      "\n",
      "Text: 'This dish, my dear friend, is nothing short of a culinary catastrophe. I've never encountered anything quite so disastrous in all my years in the kitchen. The flavors are an absolute train wreck, and the texture is reminiscent of a soggy, overcooked sponge. If this were a competition, it would undoubtedly be a contestant for the 'Worst Dish of the Century' award. An absolute travesty to the world of cuisine!'\n",
      "Predicted label: Negative \n",
      "\n",
      "Text: 'The culinary delights on offer here are simply extraordinary, with each bite offering an explosion of exquisite flavors that dance harmoniously on the palate, making it an unforgettable and truly gastronomic experience.'\n",
      "Predicted label: Positive \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# complexity - BERT\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "new_model = load_model(bert_path, custom_objects={'TFBertModel': TFBertModel})\n",
    "\n",
    "# Tokenize the input sentence into a word list, then encode each word in this list\n",
    "result_input_ids,result_attention_masks = bert_encode([test1,test2,test3],Maximum_Length)\n",
    "\n",
    "# To tesnor format\n",
    "input_ids = tf.constant(result_input_ids)\n",
    "attention_mask = tf.constant(result_attention_masks)\n",
    "\n",
    "# Feed the encoded lsit into model and predict \n",
    "label_map = {0: \"Negative\", 1: \"Positive\"}\n",
    "prob = new_model([input_ids, attention_mask])\n",
    "predicted_labels = (prob.numpy() > 0.5).astype(int)\n",
    "\n",
    "for i, label in enumerate(predicted_labels):\n",
    "    test_text = [test1, test2, test3][i]\n",
    "    print(f\"Text: '{test_text}'\")\n",
    "    print(\"Predicted label:\", label_map[label[0]], \"\\n\")\n",
    "    \n",
    "Bert_time = time.time()-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "654993a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\",do_lower_case=True) \n",
    "Maximum_Length = 64\n",
    "def bert_encode(data,maximum_length) :\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "  \n",
    "\n",
    "    for i in range(len(data)):\n",
    "        encoded = tokenizer.encode_plus(\n",
    "          data[i],\n",
    "          add_special_tokens=True, # add  [CLS] or [SEP]\n",
    "          max_length=maximum_length,\n",
    "          padding='max_length',\n",
    "          truncation=True,\n",
    "          return_attention_mask=True,\n",
    "        )\n",
    "        \n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return np.array(input_ids),np.array(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6b799a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'Chefs are nutters. They’re all self-obsessed, delicate, dainty, insecure little souls, and absolute psychopaths. Every last one of them. My gran could do better! And she’s dead!'\n",
      "Predicted label: Negative \n",
      "\n",
      "Text: 'This dish, my dear friend, is nothing short of a culinary catastrophe. I've never encountered anything quite so disastrous in all my years in the kitchen. The flavors are an absolute train wreck, and the texture is reminiscent of a soggy, overcooked sponge. If this were a competition, it would undoubtedly be a contestant for the 'Worst Dish of the Century' award. An absolute travesty to the world of cuisine!'\n",
      "Predicted label: Negative \n",
      "\n",
      "Text: 'The culinary delights on offer here are simply extraordinary, with each bite offering an explosion of exquisite flavors that dance harmoniously on the palate, making it an unforgettable and truly gastronomic experience.'\n",
      "Predicted label: Positive \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# complexity - DistillBERT\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "new_model = load_model(distillbert_path, custom_objects={'TFDistilBertModel': TFDistilBertModel})\n",
    "\n",
    "# Tokenize the input sentence into a word list, then encode each word in this list\n",
    "result_input_ids,result_attention_masks = bert_encode([test1,test2,test3],Maximum_Length)\n",
    "\n",
    "# To tesnor format\n",
    "input_ids = tf.constant(result_input_ids)\n",
    "attention_mask = tf.constant(result_attention_masks)\n",
    "\n",
    "# Feed the encoded lsit into model and predict \n",
    "label_map = {0: \"Negative\", 1: \"Positive\"}\n",
    "prob = new_model([input_ids, attention_mask])\n",
    "predicted_labels = (prob.numpy() > 0.5).astype(int)\n",
    "\n",
    "for i, label in enumerate(predicted_labels):\n",
    "    test_text = [test1, test2, test3][i]\n",
    "    print(f\"Text: '{test_text}'\")\n",
    "    print(\"Predicted label:\", label_map[label[0]], \"\\n\")\n",
    "    \n",
    "DistillBert_time = time.time()-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8de2ec7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time of Bert: 3.9437601566314697 seconds\n",
      "Total execution time of DistillBert: 1.658919334411621 seconds\n",
      "Bert Model Size: 1309895280 bytes\n",
      "DistilBert Model Size: 796775920 bytes\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total execution time of Bert: {Bert_time} seconds\")\n",
    "print(f\"Total execution time of DistillBert: {DistillBert_time} seconds\")\n",
    "\n",
    "print(f\"Bert Model Size: {bert_size} bytes\")\n",
    "print(f\"DistilBert Model Size: {distillbert_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d807e29f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
